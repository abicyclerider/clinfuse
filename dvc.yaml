vars:
- params.yaml

stages:
  # ── Docker build stages ──────────────────────────────────────────────
  build_synthea:
    cmd: >-
      mkdir -p .build &&
      docker build -t synthea -t synthea:$(git rev-parse --short HEAD) synthea_runner/synthea-docker/ &&
      docker image inspect synthea --format '{{.Id}}' > .build/synthea.iid
    deps:
    - synthea_runner/synthea-docker/Dockerfile
    outs:
    - .build/synthea.iid:
        cache: false

  build_augmentation:
    cmd: >-
      mkdir -p .build &&
      docker build -t augmentation -t augmentation:$(git rev-parse --short HEAD) augmentation/ &&
      docker image inspect augmentation --format '{{.Id}}' > .build/augmentation.iid
    deps:
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/csv_to_parquet.py
    - augmentation/cli/segment.py
    - augmentation/cli/inject_errors.py
    - augmentation/core/streaming.py
    - augmentation/config/default_config.yaml
    outs:
    - .build/augmentation.iid:
        cache: false

  build_entity_resolution:
    cmd: >-
      mkdir -p .build &&
      docker build -f entity_resolution/Dockerfile -t entity_resolution -t entity_resolution:$(git rev-parse --short HEAD) . &&
      docker image inspect entity_resolution --format '{{.Id}}' > .build/entity_resolution.iid
    deps:
    - entity_resolution/Dockerfile
    - entity_resolution/requirements-runtime.txt
    - entity_resolution/core/
    - shared/
    outs:
    - .build/entity_resolution.iid:
        cache: false

  build_prepare_dataset:
    cmd: >-
      mkdir -p .build &&
      docker build -f llm_classifier/Dockerfile.prepare -t prepare-dataset -t prepare-dataset:$(git rev-parse --short HEAD) . &&
      docker image inspect prepare-dataset --format '{{.Id}}' > .build/prepare-dataset.iid
    deps:
    - llm_classifier/Dockerfile.prepare
    - llm_classifier/requirements-prepare.txt
    - shared/summarize.py
    - shared/data_loader.py
    - shared/ground_truth.py
    - shared/medical_records.py
    outs:
    - .build/prepare-dataset.iid:
        cache: false

  # ── Production pipeline ──────────────────────────────────────────────
  generate:
    cmd: >-
      rm -rf synthea_runner/output/synthea_raw &&
      docker run --rm
      -e JAVA_OPTS=-Xmx8g
      -v $(pwd)/synthea_runner/output/synthea_raw:/synthea/synthea/output
      synthea
      /bin/sh -c "./run_synthea -p ${generate.population} -s ${generate.seed}
      --exporter.baseDirectory=./output --exporter.csv.export=true
      --exporter.fhir.export=false Massachusetts"
    params:
    - generate.population
    - generate.seed
    deps:
    - .build/synthea.iid
    outs:
    - synthea_runner/output/synthea_raw/csv:
        cache: false

  csv_to_parquet:
    cmd: >-
      rm -rf synthea_runner/output/synthea_parquet &&
      docker run --rm --entrypoint python
      -v $(pwd)/synthea_runner/output/synthea_raw/csv:/data/input:ro
      -v $(pwd)/synthea_runner/output/synthea_parquet:/data/output
      augmentation
      -m augmentation.cli.csv_to_parquet
      --input /data/input --output /data/output
    deps:
    - synthea_runner/output/synthea_raw/csv
    - .build/augmentation.iid
    outs:
    - synthea_runner/output/synthea_parquet:
        cache: false

  segment:
    cmd: >-
      rm -rf output/segmented &&
      docker run --rm --entrypoint python
      -v $(pwd)/synthea_runner/output/synthea_parquet:/data/input:ro
      -v $(pwd)/output/segmented:/data/output
      augmentation
      -m augmentation.cli.segment
      --input /data/input --output /data/output
      --assignment-seed ${augment.assignment_seed}
    params:
    - augment.assignment_seed
    deps:
    - synthea_runner/output/synthea_parquet
    - .build/augmentation.iid
    outs:
    - output/segmented:
        cache: false

  inject_errors:
    cmd: >-
      rm -rf output/augmented &&
      docker run --rm --entrypoint python
      -v $(pwd)/output/segmented:/data/input:ro
      -v $(pwd)/output/augmented:/data/output
      augmentation
      -m augmentation.cli.inject_errors
      --input /data/input --output /data/output
      --error-seed ${augment.error_seed}
    params:
    - augment.error_seed
    deps:
    - output/segmented
    - .build/augmentation.iid
    outs:
    - output/augmented:
        cache: false

  resolve:
    cmd: >-
      rm -rf output/resolved &&
      docker run --rm
      -v $(pwd)/output/augmented:/data/augmented:ro
      -v $(pwd)/output/resolved:/data/resolved
      entity_resolution
      python -m entity_resolution.resolve
      --augmented-dir /data/augmented
      --output-dir /data/resolved
      --config entity_resolution/config/matching_config.yaml
    params:
    - resolve.auto_match_probability
    - resolve.auto_reject_probability
    deps:
    - output/augmented
    - .build/entity_resolution.iid
    - entity_resolution/resolve.py
    - entity_resolution/config/matching_config.yaml
    outs:
    - output/resolved:
        cache: false

  infer:
    cmd: >-
      rm -rf output/inferred &&
      mkdir -p output/inferred &&
      llm_classifier/infer_remote.sh
      output/resolved/gray_zone_pairs.parquet
      output/inferred/predictions.parquet
    deps:
    - llm_classifier/Dockerfile
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/infer_remote.sh
    - llm_classifier/infer_classifier.py
    - llm_classifier/requirements.txt
    - llm_classifier/launch_pod.sh
    - output/resolved/gray_zone_pairs.parquet
    outs:
    - output/inferred/predictions.parquet:
        cache: false

  golden_records:
    cmd: >-
      rm -rf output/golden_records &&
      docker run --rm
      -v $(pwd)/output/augmented:/data/augmented:ro
      -v $(pwd)/output/resolved:/data/resolved:ro
      -v $(pwd)/output/inferred:/data/inferred:ro
      -v $(pwd)/output/golden_records:/data/golden_records
      entity_resolution
      python -m entity_resolution.build_golden_records
      --augmented-dir /data/augmented
      --auto-matches /data/resolved/auto_matches.parquet
      --predictions /data/inferred/predictions.parquet
      --features /data/resolved/features.parquet
      --output-dir /data/golden_records
      --config entity_resolution/config/matching_config.yaml
    deps:
    - output/resolved/auto_matches.parquet
    - output/resolved/features.parquet
    - output/inferred/predictions.parquet
    - output/augmented
    - .build/entity_resolution.iid
    - entity_resolution/build_golden_records.py
    - entity_resolution/core/
    - shared/
    outs:
    - output/golden_records:
        cache: false

  # ── Training pipeline ────────────────────────────────────────────────
  generate_training:
    cmd: >-
      rm -rf output/training/synthea_raw &&
      docker run --rm
      -e JAVA_OPTS=-Xmx8g
      -v $(pwd)/output/training/synthea_raw:/synthea/synthea/output
      synthea
      /bin/sh -c "./run_synthea -p ${generate_training.population} -s ${generate_training.seed}
      --exporter.baseDirectory=./output --exporter.csv.export=true
      --exporter.fhir.export=false Massachusetts"
    params:
    - generate_training.population
    - generate_training.seed
    deps:
    - .build/synthea.iid
    outs:
    - output/training/synthea_raw/csv:
        cache: false

  csv_to_parquet_training:
    cmd: >-
      rm -rf output/training/synthea_parquet &&
      docker run --rm --entrypoint python
      -v $(pwd)/output/training/synthea_raw/csv:/data/input:ro
      -v $(pwd)/output/training/synthea_parquet:/data/output
      augmentation
      -m augmentation.cli.csv_to_parquet
      --input /data/input --output /data/output
    deps:
    - output/training/synthea_raw/csv
    - .build/augmentation.iid
    outs:
    - output/training/synthea_parquet:
        cache: false

  segment_training:
    cmd: >-
      rm -rf output/training/segmented &&
      docker run --rm --entrypoint python
      -v $(pwd)/output/training/synthea_parquet:/data/input:ro
      -v $(pwd)/output/training/segmented:/data/output
      augmentation
      -m augmentation.cli.segment
      --input /data/input --output /data/output
      --assignment-seed ${augment_training.assignment_seed}
    params:
    - augment_training.assignment_seed
    deps:
    - output/training/synthea_parquet
    - .build/augmentation.iid
    outs:
    - output/training/segmented:
        cache: false

  inject_errors_training:
    cmd: >-
      rm -rf output/training/augmented &&
      docker run --rm --entrypoint python
      -v $(pwd)/output/training/segmented:/data/input:ro
      -v $(pwd)/output/training/augmented:/data/output
      augmentation
      -m augmentation.cli.inject_errors
      --input /data/input --output /data/output
      --error-seed ${augment_training.error_seed}
    params:
    - augment_training.error_seed
    deps:
    - output/training/segmented
    - .build/augmentation.iid
    outs:
    - output/training/augmented:
        cache: false

  prepare_dataset:
    cmd: >-
      rm -rf output/training/dataset &&
      mkdir -p output/training/dataset &&
      docker run --rm
      -v $(pwd)/output/training/augmented:/data/augmented:ro
      -v $(pwd)/output/training/dataset:/data/dataset
      -v $(pwd)/llm_classifier/.env:/app/.env:ro
      prepare-dataset
      python llm_classifier/prepare_dataset.py
      --augmented-dir /data/augmented
      --output-dir /data/dataset
      --seed ${prepare_dataset.seed}
    params:
    - prepare_dataset.seed
    deps:
    - output/training/augmented
    - .build/prepare-dataset.iid
    - llm_classifier/prepare_dataset.py
    outs:
    - output/training/dataset/dataset_info.json:
        cache: false

  train:
    cmd: >-
      rm -rf output/training/train &&
      mkdir -p output/training/train &&
      llm_classifier/train_remote.sh
      --gpu-type "NVIDIA H100 80GB HBM3"
      output/training/train
      --
      --epochs ${train.epochs}
      --batch-size ${train.batch_size}
      --lr ${train.lr}
      --max-length ${train.max_length}
      --lora-r ${train.lora_r}
    params:
    - train.epochs
    - train.batch_size
    - train.lr
    - train.max_length
    - train.lora_r
    deps:
    - output/training/dataset/dataset_info.json
    - llm_classifier/train_classifier.py
    - llm_classifier/train_remote.sh
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/launch_pod.sh
    - llm_classifier/Dockerfile
    - llm_classifier/requirements.txt
    outs:
    - output/training/train/train_metrics.json:
        cache: false

  export:
    cmd: >-
      rm -rf output/training/export &&
      mkdir -p output/training/export &&
      llm_classifier/export_remote.sh
      --gpu-type "NVIDIA H100 80GB HBM3"
      output/training/export
    deps:
    - output/training/train/train_metrics.json
    - llm_classifier/export_model.py
    - llm_classifier/export_remote.sh
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/launch_pod.sh
    - llm_classifier/Dockerfile
    - llm_classifier/requirements.txt
    outs:
    - output/training/export/export_info.json:
        cache: false
