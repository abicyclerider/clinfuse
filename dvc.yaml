vars:
- params.yaml

stages:
  # ── Docker build stages ──────────────────────────────────────────────
  build_synthea:
    cmd: >-
      mkdir -p .build &&
      docker build -t synthea -t synthea:$(git rev-parse --short HEAD) synthea_runner/synthea-docker/ &&
      docker image inspect synthea --format '{{.Id}}' > .build/synthea.iid
    deps:
    - synthea_runner/synthea-docker/Dockerfile
    outs:
    - .build/synthea.iid:
        cache: false

  # NOTE: build_augmentation removed — each augmentation stage now builds its
  # own Docker image inline (cached, ~1s) and depends only on the source files
  # it actually uses. This avoids re-running csv_to_parquet when only
  # inject_errors.py changes.

  # NOTE: build_entity_resolution removed — each entity resolution stage now
  # builds its own Docker image inline (cached, ~1s) and depends only on the
  # source files it actually uses. This avoids re-running golden_records when
  # only resolve.py changes (and vice versa).

  build_gpu_image:
    cmd: >-
      mkdir -p .build &&
      docker buildx imagetools inspect
      ghcr.io/abicyclerider/medgemma-pipeline:sha-$(git rev-parse HEAD)
      > /dev/null 2>&1 &&
      echo "sha-$(git rev-parse HEAD)" > .build/gpu-image.iid ||
      { echo "ERROR: GHCR image not found for commit $(git rev-parse --short HEAD).";
      echo "Push to main and wait for CI to build the image."; exit 1; }
    deps:
    - llm_classifier/Dockerfile
    - llm_classifier/requirements.txt
    - llm_classifier/_runpod.py
    - llm_classifier/prepare_base_model.py
    - llm_classifier/train_classifier.py
    - llm_classifier/export_model.py
    - llm_classifier/infer_classifier.py
    outs:
    - .build/gpu-image.iid:
        cache: false

  build_prepare_dataset:
    cmd: >-
      mkdir -p .build &&
      docker build -f llm_classifier/Dockerfile.prepare -t prepare-dataset -t prepare-dataset:$(git rev-parse --short HEAD) . &&
      docker image inspect prepare-dataset --format '{{.Id}}' > .build/prepare-dataset.iid
    deps:
    - llm_classifier/Dockerfile.prepare
    - llm_classifier/requirements-prepare.txt
    - shared/summarize.py
    - shared/data_loader.py
    - shared/ground_truth.py
    - shared/medical_records.py
    outs:
    - .build/prepare-dataset.iid:
        cache: false

  # ── Production pipeline ──────────────────────────────────────────────
  generate:
    cmd: >-
      rm -rf synthea_runner/output/synthea_raw &&
      docker run --rm
      -e JAVA_OPTS=-Xmx8g
      -v $(pwd)/synthea_runner/output/synthea_raw:/synthea/synthea/output
      synthea
      /bin/sh -c "./run_synthea -p ${generate.population} -s ${generate.seed}
      --exporter.baseDirectory=./output --exporter.csv.export=true
      --exporter.fhir.export=false Massachusetts"
    params:
    - generate.population
    - generate.seed
    deps:
    - .build/synthea.iid
    outs:
    - synthea_runner/output/synthea_raw/csv:
        cache: false

  csv_to_parquet:
    cmd: >-
      rm -rf synthea_runner/output/synthea_parquet &&
      docker build -t augmentation augmentation/ &&
      docker run --rm --entrypoint python
      -v $(pwd)/synthea_runner/output/synthea_raw/csv:/data/input:ro
      -v $(pwd)/synthea_runner/output/synthea_parquet:/data/output
      augmentation
      -m augmentation.cli.csv_to_parquet
      --input /data/input --output /data/output
    deps:
    - synthea_runner/output/synthea_raw/csv
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/csv_to_parquet.py
    - augmentation/core/streaming.py
    outs:
    - synthea_runner/output/synthea_parquet:
        cache: false

  segment:
    cmd: >-
      rm -rf output/segmented &&
      docker build -t augmentation augmentation/ &&
      docker run --rm --entrypoint python
      -v $(pwd)/synthea_runner/output/synthea_parquet:/data/input:ro
      -v $(pwd)/output/segmented:/data/output
      augmentation
      -m augmentation.cli.segment
      --input /data/input --output /data/output
      --assignment-seed ${augment.assignment_seed}
      --confusable-pct ${augment.confusable_pct}
    params:
    - augment.assignment_seed
    - augment.confusable_pct
    deps:
    - synthea_runner/output/synthea_parquet
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/segment.py
    - augmentation/core/streaming.py
    - augmentation/core/confusable_groups.py
    outs:
    - output/segmented:
        cache: false

  inject_errors:
    cmd: >-
      rm -rf output/augmented &&
      docker build -t augmentation augmentation/ &&
      docker run --rm --entrypoint python
      -v $(pwd)/output/segmented:/data/input:ro
      -v $(pwd)/output/augmented:/data/output
      augmentation
      -m augmentation.cli.inject_errors
      --input /data/input --output /data/output
      --error-seed ${augment.error_seed}
      --min-errors ${augment.min_errors}
      --max-errors ${augment.max_errors}
      --error-weights '${augment.error_weights}'
    params:
    - augment.error_seed
    - augment.min_errors
    - augment.max_errors
    - augment.error_weights
    deps:
    - output/segmented
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/inject_errors.py
    - augmentation/core/
    - augmentation/config/default_config.yaml
    outs:
    - output/augmented:
        cache: false

  resolve:
    cmd: >-
      rm -rf output/resolved &&
      docker build -f entity_resolution/Dockerfile -t entity_resolution . &&
      docker run --rm
      -v $(pwd)/output/augmented:/data/augmented:ro
      -v $(pwd)/output/resolved:/data/resolved
      entity_resolution
      python -m entity_resolution.resolve
      --augmented-dir /data/augmented
      --output-dir /data/resolved
      --config entity_resolution/config/matching_config.yaml
    params:
    - resolve.auto_match_probability
    - resolve.auto_reject_probability
    deps:
    - output/augmented
    - entity_resolution/Dockerfile
    - entity_resolution/requirements-runtime.txt
    - entity_resolution/resolve.py
    - entity_resolution/core/
    - entity_resolution/config/matching_config.yaml
    - shared/data_loader.py
    - shared/ground_truth.py
    - shared/medical_records.py
    - shared/summarize.py
    outs:
    - output/resolved:
        cache: false

  infer:
    cmd: >-
      mkdir -p output/inferred &&
      llm_classifier/infer_remote.sh
      --gpu-type "NVIDIA H100 80GB HBM3"
      --batch-size 64
      output/resolved/gray_zone_pairs.parquet
      output/inferred/predictions.parquet
    deps:
    - .build/gpu-image.iid
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/infer_remote.sh
    - llm_classifier/launch_pod.sh
    - output/resolved/gray_zone_pairs.parquet
    outs:
    - output/inferred/predictions.parquet:
        cache: false

  golden_records:
    cmd: >-
      rm -rf output/golden_records &&
      docker build -f entity_resolution/Dockerfile -t entity_resolution . &&
      docker run --rm
      -v $(pwd)/output/augmented:/data/augmented:ro
      -v $(pwd)/output/resolved:/data/resolved:ro
      -v $(pwd)/output/inferred:/data/inferred:ro
      -v $(pwd)/output/golden_records:/data/golden_records
      entity_resolution
      python -m entity_resolution.build_golden_records
      --augmented-dir /data/augmented
      --auto-matches /data/resolved/auto_matches.parquet
      --predictions /data/inferred/predictions.parquet
      --features /data/resolved/features.parquet
      --output-dir /data/golden_records
      --config entity_resolution/config/matching_config.yaml
    deps:
    - output/resolved/auto_matches.parquet
    - output/resolved/features.parquet
    - output/inferred/predictions.parquet
    - output/augmented
    - entity_resolution/Dockerfile
    - entity_resolution/requirements-runtime.txt
    - entity_resolution/build_golden_records.py
    - entity_resolution/core/
    - shared/data_loader.py
    - shared/ground_truth.py
    - shared/evaluation.py
    outs:
    - output/golden_records:
        cache: false

  # ── Training pipeline ────────────────────────────────────────────────
  generate_training:
    cmd: >-
      rm -rf output/training/synthea_raw &&
      docker run --rm
      -e JAVA_OPTS=-Xmx8g
      -v $(pwd)/output/training/synthea_raw:/synthea/synthea/output
      synthea
      /bin/sh -c "./run_synthea -p ${generate_training.population} -s ${generate_training.seed}
      --exporter.baseDirectory=./output --exporter.csv.export=true
      --exporter.fhir.export=false Massachusetts"
    params:
    - generate_training.population
    - generate_training.seed
    deps:
    - .build/synthea.iid
    outs:
    - output/training/synthea_raw/csv:
        cache: false

  csv_to_parquet_training:
    cmd: >-
      rm -rf output/training/synthea_parquet &&
      docker build -t augmentation augmentation/ &&
      docker run --rm --entrypoint python
      -v $(pwd)/output/training/synthea_raw/csv:/data/input:ro
      -v $(pwd)/output/training/synthea_parquet:/data/output
      augmentation
      -m augmentation.cli.csv_to_parquet
      --input /data/input --output /data/output
    deps:
    - output/training/synthea_raw/csv
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/csv_to_parquet.py
    - augmentation/core/streaming.py
    outs:
    - output/training/synthea_parquet:
        cache: false

  segment_training:
    cmd: >-
      rm -rf output/training/segmented &&
      docker build -t augmentation augmentation/ &&
      docker run --rm --entrypoint python
      -v $(pwd)/output/training/synthea_parquet:/data/input:ro
      -v $(pwd)/output/training/segmented:/data/output
      augmentation
      -m augmentation.cli.segment
      --input /data/input --output /data/output
      --assignment-seed ${augment_training.assignment_seed}
      --confusable-pct ${augment_training.confusable_pct}
    params:
    - augment_training.assignment_seed
    - augment_training.confusable_pct
    deps:
    - output/training/synthea_parquet
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/segment.py
    - augmentation/core/streaming.py
    - augmentation/core/confusable_groups.py
    outs:
    - output/training/segmented:
        cache: false

  inject_errors_training:
    cmd: >-
      rm -rf output/training/augmented &&
      docker build -t augmentation augmentation/ &&
      docker run --rm --entrypoint python
      -v $(pwd)/output/training/segmented:/data/input:ro
      -v $(pwd)/output/training/augmented:/data/output
      augmentation
      -m augmentation.cli.inject_errors
      --input /data/input --output /data/output
      --error-seed ${augment_training.error_seed}
      --min-errors ${augment_training.min_errors}
      --max-errors ${augment_training.max_errors}
      --error-weights '${augment_training.error_weights}'
    params:
    - augment_training.error_seed
    - augment_training.min_errors
    - augment_training.max_errors
    - augment_training.error_weights
    deps:
    - output/training/segmented
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/inject_errors.py
    - augmentation/core/
    - augmentation/config/default_config.yaml
    outs:
    - output/training/augmented:
        cache: false

  prepare_dataset:
    cmd: >-
      rm -rf output/training/dataset &&
      mkdir -p output/training/dataset &&
      docker run --rm
      -v $(pwd)/output/training/augmented:/data/augmented:ro
      -v $(pwd)/output/training/dataset:/data/dataset
      -v $(pwd)/llm_classifier/.env:/app/.env:ro
      prepare-dataset
      python llm_classifier/prepare_dataset.py
      --augmented-dir /data/augmented
      --output-dir /data/dataset
      --seed ${prepare_dataset.seed}
      --chunk-size 2000
    params:
    - prepare_dataset.seed
    deps:
    - output/training/augmented
    - .build/prepare-dataset.iid
    - llm_classifier/prepare_dataset.py
    outs:
    - output/training/dataset/dataset_info.json:
        cache: false

  train:
    cmd: >-
      mkdir -p output/training/train &&
      llm_classifier/train_remote.sh
      --gpu-type "NVIDIA H100 80GB HBM3"
      output/training/train
      --
      --epochs ${train.epochs}
      --batch-size ${train.batch_size}
      --grad-accum ${train.grad_accum}
      --lr ${train.lr}
      --max-length ${train.max_length}
      --lora-r ${train.lora_r}
      --max-samples ${train.max_samples}
      --max-steps ${train.max_steps}
      --gradient-checkpointing
    params:
    - train.epochs
    - train.batch_size
    - train.grad_accum
    - train.lr
    - train.max_length
    - train.lora_r
    - train.max_samples
    - train.max_steps
    deps:
    - output/training/dataset/dataset_info.json
    - .build/gpu-image.iid
    - llm_classifier/train_remote.sh
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/launch_pod.sh
    - llm_classifier/promote_model.py
    outs:
    - output/training/train/train_metrics.json:
        cache: false
    - output/training/train/promotion_decision.json:
        cache: false

  export:
    cmd: >-
      mkdir -p output/training/export &&
      llm_classifier/export_remote.sh
      --gpu-type "NVIDIA H100 80GB HBM3"
      output/training/export
    deps:
    - output/training/train/train_metrics.json
    - output/training/train/promotion_decision.json
    - .build/gpu-image.iid
    - llm_classifier/export_remote.sh
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/launch_pod.sh
    outs:
    - output/training/export/export_info.json:
        cache: false
